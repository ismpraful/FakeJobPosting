{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":90,"outputs":[{"output_type":"stream","text":"/kaggle/input/real-or-fake-fake-jobposting-prediction/fake_job_postings.csv\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nimport nltk\nfrom nltk.stem import PorterStemmer\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import roc_auc_score\n","execution_count":91,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Reading Data in the dataframe\ndf_initial = pd.read_csv(r'/kaggle/input/real-or-fake-fake-jobposting-prediction/fake_job_postings.csv')\n\n# df_initial.head()","execution_count":92,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating df with selected/important columns\ndf = df_initial[['title', 'location', 'company_profile', 'description', 'requirements', 'telecommuting', 'has_company_logo', 'has_questions', 'industry', 'fraudulent']]\n","execution_count":93,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Converting boolean values into String values by replacing 0 with negation String and replacing 1 by Positive String\ndf['telecommuting'] = df[['telecommuting']].apply(lambda x: 'telecommunicationPresent' if (x['telecommuting'] == 1) else 'telecommunicationAbsent', axis = 1)\ndf['has_company_logo'] = df[['has_company_logo']].apply(lambda x: 'logoPresent' if (x['has_company_logo'] == 1) else 'logoAbsent', axis = 1)\ndf['has_questions'] = df[['has_questions']].apply(lambda x: 'questionsPresent' if (x['has_questions'] == 1) else 'questionsAbsent', axis = 1)\n","execution_count":94,"outputs":[{"output_type":"stream","text":"/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  \"\"\"Entry point for launching an IPython kernel.\n/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  \n/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  This is separate from the ipykernel package so we can avoid doing imports until\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Filling rows having NaN values with empty string\ndf_new = df.fillna('')","execution_count":95,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Combining values of all features in a single column in order to use CountVectorizer and TfidfVectorizer\ndf_new['combined_features'] = df_new[['title', 'location', 'company_profile', 'description', 'requirements', 'telecommuting', 'has_company_logo', 'has_questions', 'industry']].apply(lambda x: ' '.join(x), axis = 1).str.lower()","execution_count":96,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Splliting words into tokens\ntokenizer = nltk.RegexpTokenizer(r\"\\w+\")\ndf_new['combined_features'] = df_new[['combined_features']].apply(lambda x: tokenizer.tokenize(x['combined_features']), axis=1)","execution_count":97,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# helper function to return stemmed words list from a list of unstemmed words\nps = PorterStemmer()\ndef get_stemmed_words_list(unstemmed_list):\n    str1 = \" \"\n    stemmed_list = []\n    for word in unstemmed_list:\n        if word.isalpha():\n            stemmed_list.append(ps.stem(word))\n    return str1.join(stemmed_list)","execution_count":98,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Stemming words of column 'combined_features'\ndf_new['combined_features'] = df_new[['combined_features']].apply(lambda x: get_stemmed_words_list(x['combined_features']), axis=1)","execution_count":99,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Defining features and target\nX = df_new[['combined_features']]\ny = df_new['fraudulent']\n\n# Spliting data into training set and test set\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size = 0.8, random_state = 1)","execution_count":100,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Defining CounterVectorizer\ncount_vect = CountVectorizer(min_df=3, ngram_range=[1, 2], stop_words='english').fit(X_train['combined_features'])\nX_train_count_vect = count_vect.transform(X_train['combined_features'])\nX_test_count_vect = count_vect.transform(X_test['combined_features'])\n\n# Gives the number of features\nlen(count_vect.get_feature_names())","execution_count":110,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Defining TfidfVectorizer\ntfidf_vect = TfidfVectorizer(min_df=3, ngram_range=[1, 2], stop_words='english').fit(X_train['combined_features'])\nX_train_tfidf_vect = tfidf_vect.transform(X_train['combined_features'])\nX_test_tfidf_vect = tfidf_vect.transform(X_test['combined_features'])\n\n# Gives the number of features\nlen(tfidf_vect.get_feature_names())","execution_count":112,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Using LogisticRegression and CounterVectorizer for calculating roc_auc_score\n\nlr_cv_model = LogisticRegression(C=0.1, max_iter=500, random_state = 1).fit(X_train_count_vect, y_train)\ny_pred_lr_cv = lr_cv_model.predict(X_test_count_vect)\nscore_lr_cv = roc_auc_score(y_test, y_pred_lr_cv)\nprint(\"roc_auc_score using LogisticRegression and CounterVectorizer: \", score_lr_cv)\n\n# get the feature names as numpy array\nfeature_names = np.array(count_vect.get_feature_names())\n# Sort the coefficients from the model\nsorted_coef_index = lr_cv_model.coef_[0].argsort()\n# Find the 10 smallest and 10 largest coefficients\nprint('\\nSmallest Coefs:\\n{}\\n'.format(feature_names[sorted_coef_index[:10]]))\nprint('Largest Coefs: \\n{}'.format(feature_names[sorted_coef_index[:-11:-1]]))","execution_count":114,"outputs":[{"output_type":"stream","text":"roc_auc_score using LogisticRegression and CounterVectorizer:  0.8681124801658905\n\n\nSmallest Coefs:\n['logopres' 'english' 'logopres questionsabs'\n 'telecommunicationabs logopres' 'gr' 'reliabl' 'live' 'sell' 'php'\n 'digit']\n\nLargest Coefs: \n['earn' 'logoabs' 'logoabs questionsabs' 'questionsabs account' 'use link'\n 'link' 'appli use' 'telecommunicationabs logoabs' 'money' 'ny']\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Using LogisticRegression and TfidfVectorizer for calculating roc_auc_score\n\nlr_tfidf_model = LogisticRegression(C=15, max_iter=500, random_state = 1).fit(X_train_tfidf_vect, y_train)\ny_pred_lr_tfidf = lr_tfidf_model.predict(X_test_tfidf_vect)\nscore_lr_tfidf = roc_auc_score(y_test, y_pred_lr_tfidf)\nprint(\"roc_auc_score using LogisticRegression and TfidfVectorizer: \", score_lr_tfidf)\n\n\n# get the feature names as numpy array\nfeature_names = np.array(tfidf_vect.get_feature_names())\n# Sort the coefficients from the model\nsorted_coef_index = lr_tfidf_model.coef_[0].argsort()\n# Find the 10 smallest and 10 largest coefficients\nprint('\\nSmallest Coefs:\\n{}\\n'.format(feature_names[sorted_coef_index[:10]]))\nprint('Largest Coefs: \\n{}'.format(feature_names[sorted_coef_index[:-11:-1]]))\n","execution_count":115,"outputs":[{"output_type":"stream","text":"roc_auc_score using LogisticRegression and TfidfVectorizer:  0.8624529566427799\n\n\nSmallest Coefs:\n['client' 'team' 'english' 'logopres questionsabs' 'logopres' 'digit'\n 'telecommunicationabs logopres' 'grow' 'softwar' 'websit']\n\nLargest Coefs: \n['logoabs' 'telecommunicationabs logoabs' 'logoabs questionsabs' 'earn'\n 'use link' 'appli use' 'data entri' 'questionsabs account' 'assist'\n 'entri']\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Using MultinomialNB and CounterVectorizer for calculating roc_auc_score\n\nmnb_cv_model = MultinomialNB(alpha=0.1).fit(X_train_count_vect, y_train)\ny_pred_mnb_cv = mnb_cv_model.predict(X_test_count_vect)\nscore_mnb_cv = roc_auc_score(y_test, y_pred_mnb_cv)\nprint(\"roc_auc_score using MultinomialNB and CounterVectorizer: \", score_mnb_cv)\n\n# get the feature names as numpy array\nfeature_names = np.array(count_vect.get_feature_names())\n# Sort the coefficients from the model\nsorted_coef_index = mnb_cv_model.coef_[0].argsort()\n# Find the 10 smallest and 10 largest coefficients\nprint('\\nSmallest Coefs:\\n{}\\n'.format(feature_names[sorted_coef_index[:10]]))\nprint('Largest Coefs: \\n{}'.format(feature_names[sorted_coef_index[:-11:-1]]))","execution_count":119,"outputs":[{"output_type":"stream","text":"roc_auc_score using MultinomialNB and CounterVectorizer:  0.9263144930773946\n\n\nSmallest Coefs:\n['limit seo' 'person type' 'person understand' 'person updat' 'person use'\n 'person user' 'person valid' 'person vehicl' 'person veri' 'person turn']\n\nLargest Coefs: \n['work' 'experi' 'manag' 'servic' 'skill' 'product' 'requir' 'amp'\n 'custom' 'develop']\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Using MultinomialNB and TfidfVectorizer for calculating roc_auc_score\n\nmnb_tfidf_model = MultinomialNB(alpha=0.01).fit(X_train_tfidf_vect, y_train)\ny_pred_mnb_tfidf = mnb_tfidf_model.predict(X_test_tfidf_vect)\nscore_mnb_tfidf = roc_auc_score(y_test, y_pred_mnb_tfidf)\nprint(\"roc_auc_score using MultinomialNB and TfidfVectorizer: \", score_mnb_tfidf)\n\n\n# get the feature names as numpy array\nfeature_names = np.array(tfidf_vect.get_feature_names())\n# Sort the coefficients from the model\nsorted_coef_index = mnb_tfidf_model.coef_[0].argsort()\n# Find the 10 smallest and 10 largest coefficients\nprint('\\nSmallest Coefs:\\n{}\\n'.format(feature_names[sorted_coef_index[:10]]))\nprint('Largest Coefs: \\n{}'.format(feature_names[sorted_coef_index[:-11:-1]]))\n","execution_count":117,"outputs":[{"output_type":"stream","text":"roc_auc_score using MultinomialNB and TfidfVectorizer:  0.8847444908433721\n\n\nSmallest Coefs:\n['limit seo' 'person type' 'person understand' 'person updat' 'person use'\n 'person user' 'person valid' 'person vehicl' 'person veri' 'person turn']\n\nLargest Coefs: \n['work' 'logoabs' 'telecommunicationabs logoabs' 'logoabs questionsabs'\n 'servic' 'data entri' 'home' 'skill' 'manag' 'custom']\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"# Conclusion\nFrom the above results based on AURUC score, it can be inferred that the best results are given by Using MultinomialNB and CounterVectorizer together."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}